{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "import datatable as dt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *\n",
    "from fastai.tabular import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './'\n",
    "TRAIN_FN = f'{path}train.feather'\n",
    "TEST_FN  = f'{path}test.feather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_identity    = dt.fread(f'{path}train_identity.csv')\n",
    "train_transaction = dt.fread(f'{path}train_transaction.csv')\n",
    "train_identity.key = \"TransactionID\"\n",
    "train = train_transaction[:, :, dt.join(train_identity)].to_pandas()\n",
    "del train_identity, train_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_identity    = dt.fread(f'{path}test_identity.csv')\n",
    "test_transaction = dt.fread(f'{path}test_transaction.csv')\n",
    "test_identity.key = \"TransactionID\"\n",
    "test = test_transaction[:, :, dt.join(test_identity)].to_pandas()\n",
    "del test_identity, test_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "    assert False\n",
    "    train = pd.read_feather(TRAIN_FN)\n",
    "except:\n",
    "    train_identity    = pd.read_csv(f'{path}train_identity.csv')\n",
    "    train_transaction = pd.read_csv(f'{path}train_transaction.csv')\n",
    "    train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "    train.to_feather(TRAIN_FN)\n",
    "    del train_identity, train_transaction\n",
    "    gc.collect()\n",
    "    \n",
    "try:\n",
    "    assert False\n",
    "    test = pd.read_feather(TEST_FN)\n",
    "except:\n",
    "    test_identity     = pd.read_csv(f'{path}test_identity.csv')\n",
    "    test_transaction  = pd.read_csv(f'{path}test_transaction.csv')\n",
    "    test  = pd.merge(test_transaction, test_identity,   on='TransactionID', how='left')\n",
    "    test.to_feather(TEST_FN)\n",
    "    del test_identity, test_transaction\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.insert(1,'isFraud', -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.insert(2, 'isTest', 1)\n",
    "train.insert(2, 'isTest', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = test['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 506691)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train, test], ignore_index= True,axis=0, sort=False)\n",
    "n_test = len(df) - n_train\n",
    "del train, test\n",
    "gc.collect()\n",
    "n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isTest</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  isTest  TransactionDT  TransactionAmt ProductCD  \\\n",
       "0        2987000        0       0          86400            68.5         W   \n",
       "1        2987001        0       0          86401            29.0         W   \n",
       "2        2987002        0       0          86469            59.0         W   \n",
       "3        2987003        0       0          86499            50.0         W   \n",
       "4        2987004        0       0          86506            50.0         H   \n",
       "\n",
       "   card1  card2  card3       card4  ...                id_31 id_32      id_33  \\\n",
       "0  13926    NaN  150.0    discover  ...                 None   NaN       None   \n",
       "1   2755  404.0  150.0  mastercard  ...                 None   NaN       None   \n",
       "2   4663  490.0  150.0        visa  ...                 None   NaN       None   \n",
       "3  18132  567.0  150.0  mastercard  ...                 None   NaN       None   \n",
       "4   4497  514.0  150.0  mastercard  ...  samsung browser 6.2  32.0  2220x1080   \n",
       "\n",
       "            id_34  id_35  id_36 id_37 id_38  DeviceType  \\\n",
       "0            None   None   None  None  None        None   \n",
       "1            None   None   None  None  None        None   \n",
       "2            None   None   None  None  None        None   \n",
       "3            None   None   None  None  None        None   \n",
       "4  match_status:2      T      F     T     T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                           None  \n",
       "1                           None  \n",
       "2                           None  \n",
       "3                           None  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 435 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#579001\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "df['dow']   = df['TransactionDT'].dt.dayofweek\n",
    "df['hour']  = df['TransactionDT'].dt.hour\n",
    "df['day']   = df['TransactionDT'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['TransactionDT', 'TransactionID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['card1_count_full'] = df['card1'].map(df['card1'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['P_emaildomain'] = df['P_emaildomain'].fillna('na')\n",
    "df['R_emaildomain'] = df['R_emaildomain'].fillna('na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['TransactionAmt_decimals'] = df['TransactionAmt'].map(lambda x:len(str(x).split('.')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_names = list(set(df.columns) - set(df._get_numeric_data().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont_names = list(np.setdiff1d(df.columns, cat_names + ['isFraud', 'isTest']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='31', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [31/31 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def diff_train_test(c):\n",
    "    df[c] = df[c].fillna('na')\n",
    "    df.loc[df[c].isnull(), c] = 'na'\n",
    "\n",
    "for c in progress_bar(cat_names): diff_train_test(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M8',\n",
       " 'id_12',\n",
       " 'DeviceType',\n",
       " 'M1',\n",
       " 'id_23',\n",
       " 'id_15',\n",
       " 'id_37',\n",
       " 'M9',\n",
       " 'card4',\n",
       " 'id_33',\n",
       " 'M7',\n",
       " 'P_emaildomain',\n",
       " 'M3',\n",
       " 'id_36',\n",
       " 'card6',\n",
       " 'id_31',\n",
       " 'id_29',\n",
       " 'DeviceInfo',\n",
       " 'id_16',\n",
       " 'id_38',\n",
       " 'ProductCD',\n",
       " 'id_34',\n",
       " 'id_27',\n",
       " 'M2',\n",
       " 'M6',\n",
       " 'R_emaildomain',\n",
       " 'M5',\n",
       " 'id_35',\n",
       " 'id_28',\n",
       " 'id_30',\n",
       " 'M4']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na                      811091\n",
       "Windows                  92710\n",
       "                         52417\n",
       "iOS Device               38502\n",
       "MacOS                    23722\n",
       "                         ...  \n",
       "Mobiistar_LAI_Yuna_X         1\n",
       "DLI-L22                      1\n",
       "Azumi_KINZO_A5_QL            1\n",
       "Azumi_DOSHI_A55_QL           1\n",
       "A462C                        1\n",
       "Name: DeviceInfo, Length: 2801, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DeviceInfo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='31', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [31/31 00:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antor/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "min_threshold = 0.01\n",
    "\n",
    "def frequency_encoding(c):\n",
    "    encoding = df.groupby(c).size()/len(df)\n",
    "    return df[c].map(encoding)\n",
    "for c in progress_bar(np.setdiff1d(cat_names,['dow_freq','hour_freq','day_freq'])):\n",
    "    freq = frequency_encoding(c)\n",
    "    df[f\"{c}_freq\"] = freq\n",
    "    cont_names.append(f\"{c}_freq\")\n",
    "    df[c][(freq <= min_threshold)] = f'm_{min_threshold}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "na             811091\n",
       "Windows         92710\n",
       "m_0.01          66459\n",
       "                52417\n",
       "iOS Device      38502\n",
       "MacOS           23722\n",
       "Trident/7.0     12330\n",
       "Name: DeviceInfo, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DeviceInfo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='31', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [31/31 00:07<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in progress_bar(cat_names):\n",
    "    df[c] = df[c].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = range(int(n_train*.8))\n",
    "valid_idx = list(np.setdiff1d(range(n_train), train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(df.iloc[train_idx][cont_names + cat_names], label=df.iloc[train_idx]['isFraud'],\n",
    "                      categorical_feature = cat_names)\n",
    "val_data = lgb.Dataset(df.iloc[valid_idx][cont_names + cat_names], label=df.iloc[valid_idx]['isFraud'],\n",
    "                      categorical_feature = cat_names)\n",
    "#clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], \n",
    "                verbose_eval=1000, early_stopping_rounds=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in progress_bar(cat_names):\n",
    "    df[c] = df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cont_names:\n",
    "    print(c, pd.isna(df[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = TabularDataBunch.from_df(path, df[:n_train], dep_var, cat_names=cat_names, \n",
    "                                cont_names =cont_names,\n",
    "                                valid_idx=valid_idx, \n",
    "                                procs=[Normalize, FillMissing, Categorify])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.batch_size = 4096*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(\n",
    "    data, layers=[512]*32,\n",
    "    ps=0.3,metrics=[accuracy, AUROC()], callback_fns=ShowGraph, use_bn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = Tensor(n_train / (2 * np.bincount(df['isFraud'][:n_train].values)))\n",
    "learn.loss_func.weight=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10,5.75e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2., alpha=.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        else: return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThresholdLoss(Module):\n",
    "    def __init__(self, pct=0.2, weight=None): \n",
    "        self.pct = pct\n",
    "        self.loss = FocalLoss(reduction='none')\n",
    "#        self.loss = nn.CrossEntropyLoss(weight=weight.cuda(),reduction='none')\n",
    "    def forward(self, from_forward, t_fraud):\n",
    "        p_fraud = from_forward\n",
    "        loss = self.loss(p_fraud, t_fraud)\n",
    "        threshold = loss.topk(int(self.pct*len(t_fraud)))[0][-1]\n",
    "        loss[loss < threshold] == 0.\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.loss_func = FocalLoss() #ThresholdLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.callbacks.append(ReduceLROnPlateauCallback(learn, monitor=\"train_loss\", patience=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1000,1.2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds,y,losses = learn.get_preds(DatasetType.Fix,with_loss=True)\n",
    "interp = ClassificationInterpretation(learn, preds, y, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.top_losses(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.top_losses(len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'D1',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'TransactionAmt',\n",
       " 'TransactionAmt_decimals',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V11',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V12',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V13',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V14',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V15',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V16',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V17',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V18',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V19',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V2',\n",
       " 'V20',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V21',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V22',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V23',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V24',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V25',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V26',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V27',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V28',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V29',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V3',\n",
       " 'V30',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V31',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V32',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V33',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V4',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V5',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V6',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V7',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V8',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V9',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'card1',\n",
       " 'card1_count_full',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card5',\n",
       " 'day',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'dow',\n",
       " 'hour',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_32',\n",
       " 'DeviceInfo_freq',\n",
       " 'DeviceType_freq',\n",
       " 'M1_freq',\n",
       " 'M2_freq',\n",
       " 'M3_freq',\n",
       " 'M4_freq',\n",
       " 'M5_freq',\n",
       " 'M6_freq',\n",
       " 'M7_freq',\n",
       " 'M8_freq',\n",
       " 'M9_freq',\n",
       " 'P_emaildomain_freq',\n",
       " 'ProductCD_freq',\n",
       " 'R_emaildomain_freq',\n",
       " 'card4_freq',\n",
       " 'card6_freq',\n",
       " 'id_12_freq',\n",
       " 'id_15_freq',\n",
       " 'id_16_freq',\n",
       " 'id_23_freq',\n",
       " 'id_27_freq',\n",
       " 'id_28_freq',\n",
       " 'id_29_freq',\n",
       " 'id_30_freq',\n",
       " 'id_31_freq',\n",
       " 'id_33_freq',\n",
       " 'id_34_freq',\n",
       " 'id_35_freq',\n",
       " 'id_36_freq',\n",
       " 'id_37_freq',\n",
       " 'id_38_freq']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cont_names = list(np.setdiff1d(cont_names, [c for c in cont_names if c.startswith('V')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cat_names = list(np.setdiff1d(cat_names, ['DeviceInfo', 'DeviceType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Categorify(cat_names, cont_names)\n",
    "c.apply_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = FillMissing(cat_names, cont_names)\n",
    "c.apply_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_columns = [c for c in df.columns if c.endswith('_na')]\n",
    "cont_names.extend(na_columns)\n",
    "cat_names = list(np.setdiff1d(cat_names, na_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'D1',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'TransactionAmt',\n",
       " 'TransactionAmt_decimals',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V11',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V12',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V13',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V14',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V15',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V16',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V17',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V18',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V19',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V2',\n",
       " 'V20',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V21',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V22',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V23',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V24',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V25',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V26',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V27',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V28',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V29',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V3',\n",
       " 'V30',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V31',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V32',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V33',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V4',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V5',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V6',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V7',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V8',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V9',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'card1',\n",
       " 'card1_count_full',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card5',\n",
       " 'day',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'dow',\n",
       " 'hour',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_32',\n",
       " 'DeviceInfo_freq',\n",
       " 'DeviceType_freq',\n",
       " 'M1_freq',\n",
       " 'M2_freq',\n",
       " 'M3_freq',\n",
       " 'M4_freq',\n",
       " 'M5_freq',\n",
       " 'M6_freq',\n",
       " 'M7_freq',\n",
       " 'M8_freq',\n",
       " 'M9_freq',\n",
       " 'P_emaildomain_freq',\n",
       " 'ProductCD_freq',\n",
       " 'R_emaildomain_freq',\n",
       " 'card4_freq',\n",
       " 'card6_freq',\n",
       " 'id_12_freq',\n",
       " 'id_15_freq',\n",
       " 'id_16_freq',\n",
       " 'id_23_freq',\n",
       " 'id_27_freq',\n",
       " 'id_28_freq',\n",
       " 'id_29_freq',\n",
       " 'id_30_freq',\n",
       " 'id_31_freq',\n",
       " 'id_33_freq',\n",
       " 'id_34_freq',\n",
       " 'id_35_freq',\n",
       " 'id_36_freq',\n",
       " 'id_37_freq',\n",
       " 'id_38_freq',\n",
       " 'C1_na',\n",
       " 'C10_na',\n",
       " 'C11_na',\n",
       " 'C12_na',\n",
       " 'C13_na',\n",
       " 'C14_na',\n",
       " 'C2_na',\n",
       " 'C3_na',\n",
       " 'C4_na',\n",
       " 'C5_na',\n",
       " 'C6_na',\n",
       " 'C7_na',\n",
       " 'C8_na',\n",
       " 'C9_na',\n",
       " 'D1_na',\n",
       " 'D10_na',\n",
       " 'D11_na',\n",
       " 'D12_na',\n",
       " 'D13_na',\n",
       " 'D14_na',\n",
       " 'D15_na',\n",
       " 'D2_na',\n",
       " 'D3_na',\n",
       " 'D4_na',\n",
       " 'D5_na',\n",
       " 'D6_na',\n",
       " 'D7_na',\n",
       " 'D8_na',\n",
       " 'D9_na',\n",
       " 'V1_na',\n",
       " 'V10_na',\n",
       " 'V100_na',\n",
       " 'V101_na',\n",
       " 'V102_na',\n",
       " 'V103_na',\n",
       " 'V104_na',\n",
       " 'V105_na',\n",
       " 'V106_na',\n",
       " 'V107_na',\n",
       " 'V108_na',\n",
       " 'V109_na',\n",
       " 'V11_na',\n",
       " 'V110_na',\n",
       " 'V111_na',\n",
       " 'V112_na',\n",
       " 'V113_na',\n",
       " 'V114_na',\n",
       " 'V115_na',\n",
       " 'V116_na',\n",
       " 'V117_na',\n",
       " 'V118_na',\n",
       " 'V119_na',\n",
       " 'V12_na',\n",
       " 'V120_na',\n",
       " 'V121_na',\n",
       " 'V122_na',\n",
       " 'V123_na',\n",
       " 'V124_na',\n",
       " 'V125_na',\n",
       " 'V126_na',\n",
       " 'V127_na',\n",
       " 'V128_na',\n",
       " 'V129_na',\n",
       " 'V13_na',\n",
       " 'V130_na',\n",
       " 'V131_na',\n",
       " 'V132_na',\n",
       " 'V133_na',\n",
       " 'V134_na',\n",
       " 'V135_na',\n",
       " 'V136_na',\n",
       " 'V137_na',\n",
       " 'V138_na',\n",
       " 'V139_na',\n",
       " 'V14_na',\n",
       " 'V140_na',\n",
       " 'V141_na',\n",
       " 'V142_na',\n",
       " 'V143_na',\n",
       " 'V144_na',\n",
       " 'V145_na',\n",
       " 'V146_na',\n",
       " 'V147_na',\n",
       " 'V148_na',\n",
       " 'V149_na',\n",
       " 'V15_na',\n",
       " 'V150_na',\n",
       " 'V151_na',\n",
       " 'V152_na',\n",
       " 'V153_na',\n",
       " 'V154_na',\n",
       " 'V155_na',\n",
       " 'V156_na',\n",
       " 'V157_na',\n",
       " 'V158_na',\n",
       " 'V159_na',\n",
       " 'V16_na',\n",
       " 'V160_na',\n",
       " 'V161_na',\n",
       " 'V162_na',\n",
       " 'V163_na',\n",
       " 'V164_na',\n",
       " 'V165_na',\n",
       " 'V166_na',\n",
       " 'V167_na',\n",
       " 'V168_na',\n",
       " 'V169_na',\n",
       " 'V17_na',\n",
       " 'V170_na',\n",
       " 'V171_na',\n",
       " 'V172_na',\n",
       " 'V173_na',\n",
       " 'V174_na',\n",
       " 'V175_na',\n",
       " 'V176_na',\n",
       " 'V177_na',\n",
       " 'V178_na',\n",
       " 'V179_na',\n",
       " 'V18_na',\n",
       " 'V180_na',\n",
       " 'V181_na',\n",
       " 'V182_na',\n",
       " 'V183_na',\n",
       " 'V184_na',\n",
       " 'V185_na',\n",
       " 'V186_na',\n",
       " 'V187_na',\n",
       " 'V188_na',\n",
       " 'V189_na',\n",
       " 'V19_na',\n",
       " 'V190_na',\n",
       " 'V191_na',\n",
       " 'V192_na',\n",
       " 'V193_na',\n",
       " 'V194_na',\n",
       " 'V195_na',\n",
       " 'V196_na',\n",
       " 'V197_na',\n",
       " 'V198_na',\n",
       " 'V199_na',\n",
       " 'V2_na',\n",
       " 'V20_na',\n",
       " 'V200_na',\n",
       " 'V201_na',\n",
       " 'V202_na',\n",
       " 'V203_na',\n",
       " 'V204_na',\n",
       " 'V205_na',\n",
       " 'V206_na',\n",
       " 'V207_na',\n",
       " 'V208_na',\n",
       " 'V209_na',\n",
       " 'V21_na',\n",
       " 'V210_na',\n",
       " 'V211_na',\n",
       " 'V212_na',\n",
       " 'V213_na',\n",
       " 'V214_na',\n",
       " 'V215_na',\n",
       " 'V216_na',\n",
       " 'V217_na',\n",
       " 'V218_na',\n",
       " 'V219_na',\n",
       " 'V22_na',\n",
       " 'V220_na',\n",
       " 'V221_na',\n",
       " 'V222_na',\n",
       " 'V223_na',\n",
       " 'V224_na',\n",
       " 'V225_na',\n",
       " 'V226_na',\n",
       " 'V227_na',\n",
       " 'V228_na',\n",
       " 'V229_na',\n",
       " 'V23_na',\n",
       " 'V230_na',\n",
       " 'V231_na',\n",
       " 'V232_na',\n",
       " 'V233_na',\n",
       " 'V234_na',\n",
       " 'V235_na',\n",
       " 'V236_na',\n",
       " 'V237_na',\n",
       " 'V238_na',\n",
       " 'V239_na',\n",
       " 'V24_na',\n",
       " 'V240_na',\n",
       " 'V241_na',\n",
       " 'V242_na',\n",
       " 'V243_na',\n",
       " 'V244_na',\n",
       " 'V245_na',\n",
       " 'V246_na',\n",
       " 'V247_na',\n",
       " 'V248_na',\n",
       " 'V249_na',\n",
       " 'V25_na',\n",
       " 'V250_na',\n",
       " 'V251_na',\n",
       " 'V252_na',\n",
       " 'V253_na',\n",
       " 'V254_na',\n",
       " 'V255_na',\n",
       " 'V256_na',\n",
       " 'V257_na',\n",
       " 'V258_na',\n",
       " 'V259_na',\n",
       " 'V26_na',\n",
       " 'V260_na',\n",
       " 'V261_na',\n",
       " 'V262_na',\n",
       " 'V263_na',\n",
       " 'V264_na',\n",
       " 'V265_na',\n",
       " 'V266_na',\n",
       " 'V267_na',\n",
       " 'V268_na',\n",
       " 'V269_na',\n",
       " 'V27_na',\n",
       " 'V270_na',\n",
       " 'V271_na',\n",
       " 'V272_na',\n",
       " 'V273_na',\n",
       " 'V274_na',\n",
       " 'V275_na',\n",
       " 'V276_na',\n",
       " 'V277_na',\n",
       " 'V278_na',\n",
       " 'V279_na',\n",
       " 'V28_na',\n",
       " 'V280_na',\n",
       " 'V281_na',\n",
       " 'V282_na',\n",
       " 'V283_na',\n",
       " 'V284_na',\n",
       " 'V285_na',\n",
       " 'V286_na',\n",
       " 'V287_na',\n",
       " 'V288_na',\n",
       " 'V289_na',\n",
       " 'V29_na',\n",
       " 'V290_na',\n",
       " 'V291_na',\n",
       " 'V292_na',\n",
       " 'V293_na',\n",
       " 'V294_na',\n",
       " 'V295_na',\n",
       " 'V296_na',\n",
       " 'V297_na',\n",
       " 'V298_na',\n",
       " 'V299_na',\n",
       " 'V3_na',\n",
       " 'V30_na',\n",
       " 'V300_na',\n",
       " 'V301_na',\n",
       " 'V302_na',\n",
       " 'V303_na',\n",
       " 'V304_na',\n",
       " 'V305_na',\n",
       " 'V306_na',\n",
       " 'V307_na',\n",
       " 'V308_na',\n",
       " 'V309_na',\n",
       " 'V31_na',\n",
       " 'V310_na',\n",
       " 'V311_na',\n",
       " 'V312_na',\n",
       " 'V313_na',\n",
       " 'V314_na',\n",
       " 'V315_na',\n",
       " 'V316_na',\n",
       " 'V317_na',\n",
       " 'V318_na',\n",
       " 'V319_na',\n",
       " 'V32_na',\n",
       " 'V320_na',\n",
       " 'V321_na',\n",
       " 'V322_na',\n",
       " 'V323_na',\n",
       " 'V324_na',\n",
       " 'V325_na',\n",
       " 'V326_na',\n",
       " 'V327_na',\n",
       " 'V328_na',\n",
       " 'V329_na',\n",
       " 'V33_na',\n",
       " 'V330_na',\n",
       " 'V331_na',\n",
       " 'V332_na',\n",
       " 'V333_na',\n",
       " 'V334_na',\n",
       " 'V335_na',\n",
       " 'V336_na',\n",
       " 'V337_na',\n",
       " 'V338_na',\n",
       " 'V339_na',\n",
       " 'V34_na',\n",
       " 'V35_na',\n",
       " 'V36_na',\n",
       " 'V37_na',\n",
       " 'V38_na',\n",
       " 'V39_na',\n",
       " 'V4_na',\n",
       " 'V40_na',\n",
       " 'V41_na',\n",
       " 'V42_na',\n",
       " 'V43_na',\n",
       " 'V44_na',\n",
       " 'V45_na',\n",
       " 'V46_na',\n",
       " 'V47_na',\n",
       " 'V48_na',\n",
       " 'V49_na',\n",
       " 'V5_na',\n",
       " 'V50_na',\n",
       " 'V51_na',\n",
       " 'V52_na',\n",
       " 'V53_na',\n",
       " 'V54_na',\n",
       " 'V55_na',\n",
       " 'V56_na',\n",
       " 'V57_na',\n",
       " 'V58_na',\n",
       " 'V59_na',\n",
       " 'V6_na',\n",
       " 'V60_na',\n",
       " 'V61_na',\n",
       " 'V62_na',\n",
       " 'V63_na',\n",
       " 'V64_na',\n",
       " 'V65_na',\n",
       " 'V66_na',\n",
       " 'V67_na',\n",
       " 'V68_na',\n",
       " 'V69_na',\n",
       " 'V7_na',\n",
       " 'V70_na',\n",
       " 'V71_na',\n",
       " 'V72_na',\n",
       " 'V73_na',\n",
       " 'V74_na',\n",
       " 'V75_na',\n",
       " 'V76_na',\n",
       " 'V77_na',\n",
       " 'V78_na',\n",
       " 'V79_na',\n",
       " 'V8_na',\n",
       " 'V80_na',\n",
       " 'V81_na',\n",
       " 'V82_na',\n",
       " 'V83_na',\n",
       " 'V84_na',\n",
       " 'V85_na',\n",
       " 'V86_na',\n",
       " 'V87_na',\n",
       " 'V88_na',\n",
       " 'V89_na',\n",
       " 'V9_na',\n",
       " 'V90_na',\n",
       " 'V91_na',\n",
       " 'V92_na',\n",
       " 'V93_na',\n",
       " 'V94_na',\n",
       " 'V95_na',\n",
       " 'V96_na',\n",
       " 'V97_na',\n",
       " 'V98_na',\n",
       " 'V99_na',\n",
       " 'addr1_na',\n",
       " 'addr2_na',\n",
       " 'card2_na',\n",
       " 'card3_na',\n",
       " 'card5_na',\n",
       " 'dist1_na',\n",
       " 'dist2_na',\n",
       " 'id_01_na',\n",
       " 'id_02_na',\n",
       " 'id_03_na',\n",
       " 'id_04_na',\n",
       " 'id_05_na',\n",
       " 'id_06_na',\n",
       " 'id_07_na',\n",
       " 'id_08_na',\n",
       " 'id_09_na',\n",
       " 'id_10_na',\n",
       " 'id_11_na',\n",
       " 'id_13_na',\n",
       " 'id_14_na',\n",
       " 'id_17_na',\n",
       " 'id_18_na',\n",
       " 'id_19_na',\n",
       " 'id_20_na',\n",
       " 'id_21_na',\n",
       " 'id_22_na',\n",
       " 'id_24_na',\n",
       " 'id_25_na',\n",
       " 'id_26_na',\n",
       " 'id_32_na',\n",
       " 'C1_na',\n",
       " 'C10_na',\n",
       " 'C11_na',\n",
       " 'C12_na',\n",
       " 'C13_na',\n",
       " 'C14_na',\n",
       " 'C2_na',\n",
       " 'C3_na',\n",
       " 'C4_na',\n",
       " 'C5_na',\n",
       " 'C6_na',\n",
       " 'C7_na',\n",
       " 'C8_na',\n",
       " 'C9_na',\n",
       " 'D1_na',\n",
       " 'D10_na',\n",
       " 'D11_na',\n",
       " 'D12_na',\n",
       " 'D13_na',\n",
       " 'D14_na',\n",
       " 'D15_na',\n",
       " 'D2_na',\n",
       " 'D3_na',\n",
       " 'D4_na',\n",
       " 'D5_na',\n",
       " 'D6_na',\n",
       " 'D7_na',\n",
       " 'D8_na',\n",
       " 'D9_na',\n",
       " 'V1_na',\n",
       " 'V10_na',\n",
       " 'V100_na',\n",
       " 'V101_na',\n",
       " 'V102_na',\n",
       " 'V103_na',\n",
       " 'V104_na',\n",
       " 'V105_na',\n",
       " 'V106_na',\n",
       " 'V107_na',\n",
       " 'V108_na',\n",
       " 'V109_na',\n",
       " 'V11_na',\n",
       " 'V110_na',\n",
       " 'V111_na',\n",
       " 'V112_na',\n",
       " 'V113_na',\n",
       " 'V114_na',\n",
       " 'V115_na',\n",
       " 'V116_na',\n",
       " 'V117_na',\n",
       " 'V118_na',\n",
       " 'V119_na',\n",
       " 'V12_na',\n",
       " 'V120_na',\n",
       " 'V121_na',\n",
       " 'V122_na',\n",
       " 'V123_na',\n",
       " 'V124_na',\n",
       " 'V125_na',\n",
       " 'V126_na',\n",
       " 'V127_na',\n",
       " 'V128_na',\n",
       " 'V129_na',\n",
       " 'V13_na',\n",
       " 'V130_na',\n",
       " 'V131_na',\n",
       " 'V132_na',\n",
       " 'V133_na',\n",
       " 'V134_na',\n",
       " 'V135_na',\n",
       " 'V136_na',\n",
       " 'V137_na',\n",
       " 'V138_na',\n",
       " 'V139_na',\n",
       " 'V14_na',\n",
       " 'V140_na',\n",
       " 'V141_na',\n",
       " 'V142_na',\n",
       " 'V143_na',\n",
       " 'V144_na',\n",
       " 'V145_na',\n",
       " 'V146_na',\n",
       " 'V147_na',\n",
       " 'V148_na',\n",
       " 'V149_na',\n",
       " 'V15_na',\n",
       " 'V150_na',\n",
       " 'V151_na',\n",
       " 'V152_na',\n",
       " 'V153_na',\n",
       " 'V154_na',\n",
       " 'V155_na',\n",
       " 'V156_na',\n",
       " 'V157_na',\n",
       " 'V158_na',\n",
       " 'V159_na',\n",
       " 'V16_na',\n",
       " 'V160_na',\n",
       " 'V161_na',\n",
       " 'V162_na',\n",
       " 'V163_na',\n",
       " 'V164_na',\n",
       " 'V165_na',\n",
       " 'V166_na',\n",
       " 'V167_na',\n",
       " 'V168_na',\n",
       " 'V169_na',\n",
       " 'V17_na',\n",
       " 'V170_na',\n",
       " 'V171_na',\n",
       " 'V172_na',\n",
       " 'V173_na',\n",
       " 'V174_na',\n",
       " 'V175_na',\n",
       " 'V176_na',\n",
       " 'V177_na',\n",
       " 'V178_na',\n",
       " 'V179_na',\n",
       " 'V18_na',\n",
       " 'V180_na',\n",
       " 'V181_na',\n",
       " 'V182_na',\n",
       " 'V183_na',\n",
       " 'V184_na',\n",
       " 'V185_na',\n",
       " 'V186_na',\n",
       " 'V187_na',\n",
       " 'V188_na',\n",
       " 'V189_na',\n",
       " 'V19_na',\n",
       " 'V190_na',\n",
       " 'V191_na',\n",
       " 'V192_na',\n",
       " 'V193_na',\n",
       " 'V194_na',\n",
       " 'V195_na',\n",
       " 'V196_na',\n",
       " 'V197_na',\n",
       " 'V198_na',\n",
       " 'V199_na',\n",
       " 'V2_na',\n",
       " 'V20_na',\n",
       " 'V200_na',\n",
       " 'V201_na',\n",
       " 'V202_na',\n",
       " 'V203_na',\n",
       " 'V204_na',\n",
       " 'V205_na',\n",
       " 'V206_na',\n",
       " 'V207_na',\n",
       " 'V208_na',\n",
       " 'V209_na',\n",
       " 'V21_na',\n",
       " 'V210_na',\n",
       " 'V211_na',\n",
       " 'V212_na',\n",
       " 'V213_na',\n",
       " 'V214_na',\n",
       " 'V215_na',\n",
       " 'V216_na',\n",
       " 'V217_na',\n",
       " 'V218_na',\n",
       " 'V219_na',\n",
       " 'V22_na',\n",
       " 'V220_na',\n",
       " 'V221_na',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeviceInfo',\n",
       " 'DeviceType',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'P_emaildomain',\n",
       " 'ProductCD',\n",
       " 'R_emaildomain',\n",
       " 'card4',\n",
       " 'card6',\n",
       " 'id_12',\n",
       " 'id_15',\n",
       " 'id_16',\n",
       " 'id_23',\n",
       " 'id_27',\n",
       " 'id_28',\n",
       " 'id_29',\n",
       " 'id_30',\n",
       " 'id_31',\n",
       " 'id_33',\n",
       " 'id_34',\n",
       " 'id_35',\n",
       " 'id_36',\n",
       " 'id_37',\n",
       " 'id_38']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [LongTensor(df[cat_name].cat.codes.values) for cat_name in cat_names]\n",
    "cats = torch.cat([cat.unsqueeze(-1) for cat in cats], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 3, 1,  ..., 2, 3, 3],\n",
       "        [6, 3, 0,  ..., 2, 3, 3],\n",
       "        [6, 3, 1,  ..., 2, 3, 3],\n",
       "        ...,\n",
       "        [6, 3, 1,  ..., 2, 3, 3],\n",
       "        [6, 3, 1,  ..., 2, 3, 3],\n",
       "        [5, 2, 0,  ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([2., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([14.,  0.,  0.,  ...,  0.,  0.,  0.]),\n",
       " tensor([13.,  0.,  0.,  ...,  0.,  0.,  0.]),\n",
       " tensor([ 13.,  69., 315.,  ...,   0.,   0.,  69.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([  0.,   0., 315.,  ...,   0.,   0.,   0.]),\n",
       " tensor([104., 104., 104.,  ..., 104., 104., 104.]),\n",
       " tensor([13.,  8.,  8.,  ...,  8.,  8.,  8.]),\n",
       " tensor([23.,  0.,  0.,  ...,  0.,  0.,  0.]),\n",
       " tensor([9., 9., 9.,  ..., 9., 9., 9.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([37.7917, 37.7917, 37.7917,  ..., 37.7917, 37.7917, 29.9583]),\n",
       " tensor([0.6667, 0.6667, 0.6667,  ..., 0.6667, 0.6667, 0.9583]),\n",
       " tensor([ 68.5000,  29.0000,  59.0000,  ...,  49.0000, 202.0000,  24.3460]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 3.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([117.,   0.,   0.,  ...,   0.,   0.,   0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([117.,   0.,   0.,  ...,   0.,   0.,   0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([117.,   0.,   0.,  ...,   0.,   0.,   0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([117.,   0.,   0.,  ...,   0.,   0.,   0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 0., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([315., 325., 330.,  ..., 327., 177., 299.]),\n",
       " tensor([87., 87., 87.,  ..., 87., 87., 87.]),\n",
       " tensor([13926.,  2755.,  4663.,  ..., 16661., 16621.,  5713.]),\n",
       " tensor([  56., 1338., 1794.,  ..., 1284.,    4.,  175.]),\n",
       " tensor([361., 404., 490.,  ..., 490., 516., 168.]),\n",
       " tensor([150., 150., 150.,  ..., 150., 150., 144.]),\n",
       " tensor([142., 102., 166.,  ..., 226., 224., 147.]),\n",
       " tensor([ 2.,  2.,  2.,  ..., 31., 31., 31.]),\n",
       " tensor([ 19.,   8., 287.,  ...,   8.,   8.,   8.]),\n",
       " tensor([41., 41., 41.,  ..., 41., 41., 41.]),\n",
       " tensor([5., 5., 5.,  ..., 0., 0., 0.]),\n",
       " tensor([ 0.,  0.,  0.,  ..., 23., 23., 23.]),\n",
       " tensor([ -5.,  -5.,  -5.,  ...,  -5.,  -5., -10.]),\n",
       " tensor([129158.5000, 129158.5000, 129158.5000,  ..., 129158.5000,\n",
       "         129158.5000, 692090.0000]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([13., 13., 13.,  ..., 13., 13., 13.]),\n",
       " tensor([-34., -34., -34.,  ..., -34., -34., -34.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([100., 100., 100.,  ..., 100., 100., 100.]),\n",
       " tensor([52., 52., 52.,  ..., 52., 52., 27.]),\n",
       " tensor([-300., -300., -300.,  ..., -300., -300., -300.]),\n",
       " tensor([166., 166., 166.,  ..., 166., 166., 225.]),\n",
       " tensor([15., 15., 15.,  ..., 15., 15., 15.]),\n",
       " tensor([337., 337., 337.,  ..., 337., 337., 266.]),\n",
       " tensor([472., 472., 472.,  ..., 472., 472., 127.]),\n",
       " tensor([277., 277., 277.,  ..., 277., 277., 277.]),\n",
       " tensor([14., 14., 14.,  ..., 14., 14., 14.]),\n",
       " tensor([11., 11., 11.,  ..., 11., 11., 11.]),\n",
       " tensor([321., 321., 321.,  ..., 321., 321., 321.]),\n",
       " tensor([147., 147., 147.,  ..., 147., 147., 147.]),\n",
       " tensor([24., 24., 24.,  ..., 24., 24., 24.]),\n",
       " tensor([7.3922e-01, 7.3922e-01, 7.3922e-01,  ..., 7.3922e-01, 7.3922e-01,\n",
       "         4.0557e-04]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1077]),\n",
       " tensor([0.5919, 0.4081, 0.5919,  ..., 0.5919, 0.5919, 0.4081]),\n",
       " tensor([0.5362, 0.4081, 0.5362,  ..., 0.5362, 0.5362, 0.4081]),\n",
       " tensor([0.4723, 0.4081, 0.4723,  ..., 0.4723, 0.4723, 0.4081]),\n",
       " tensor([0.1121, 0.3261, 0.3261,  ..., 0.3261, 0.3261, 0.1121]),\n",
       " tensor([0.2189, 0.1795, 0.2189,  ..., 0.2189, 0.2189, 0.6016]),\n",
       " tensor([0.3185, 0.3185, 0.3823,  ..., 0.3823, 0.3823, 0.2992]),\n",
       " tensor([0.5298, 0.5298, 0.4052,  ..., 0.4052, 0.4052, 0.5298]),\n",
       " tensor([0.5297, 0.5297, 0.2950,  ..., 0.1753, 0.2950, 0.5297]),\n",
       " tensor([0.5297, 0.5297, 0.0675,  ..., 0.4028, 0.0675, 0.5297]),\n",
       " tensor([0.1491, 0.3972, 0.0091,  ..., 0.0781, 0.0781, 0.0781]),\n",
       " tensor([0.7297, 0.7297, 0.7297,  ..., 0.7297, 0.7297, 0.1256]),\n",
       " tensor([0.7510, 0.7510, 0.7510,  ..., 0.7510, 0.7510, 0.0485]),\n",
       " tensor([0.0087, 0.3166, 0.6559,  ..., 0.6559, 0.3166, 0.6559]),\n",
       " tensor([0.2439, 0.2439, 0.7519,  ..., 0.7519, 0.7519, 0.2439]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.2223]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1237]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1210]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.2515]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.2515]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1384]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1360]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1257]),\n",
       " tensor([7.3922e-01, 7.3922e-01, 7.3922e-01,  ..., 7.3922e-01, 7.3922e-01,\n",
       "         2.6886e-04]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1296]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1241]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1171]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.2437]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1961]),\n",
       " tensor([0.7392, 0.7392, 0.7392,  ..., 0.7392, 0.7392, 0.1540]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 1., 0.,  ..., 0., 0., 1.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 0.]),\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts = [Tensor(df[cont_name].values) for cont_name in cont_names]\n",
    "conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conts = [Tensor(df[cont_name].values) for cont_name in progress_bar(cont_names)]\n",
    "#conts = [torch.log1p(Tensor(df[cont_name].values-df[cont_name].min())) for cont_name in cont_names]\n",
    "#for cont in conts: cont[cont!=cont] = -100 # turn NaNs into -1\n",
    "conts = torch.cat([cont.unsqueeze(-1) for cont in conts], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1097231, 1232])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12.2456,  3.6563,  8.9684,  ...,  0.9907,  0.9907,  0.8649]),\n",
       " tensor([1.1186e+02, 7.1966e+01, 8.0797e+01,  ..., 9.5832e-02, 9.6014e-02,\n",
       "         3.4185e-01]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts_mean, conts_std = conts.mean(dim=0), conts.std(dim=0)\n",
    "conts_mean, conts_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conts = (conts - conts_mean) / conts_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransactionItem(ItemBase):\n",
    "    def __init__(self,i): self.i = i\n",
    "    @property\n",
    "    def data(self):\n",
    "        return cats[self.i], conts[self.i]\n",
    "    def __str__(self): return f'{self.i}'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i)\n",
    "    \n",
    "    def reconstruct(self, t:Tensor, x:Tensor=None):\n",
    "        return t\n",
    "\n",
    "class TransactionItemList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "\n",
    "    def get(self, i):\n",
    "        return super().get(i)\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1097231, 1232])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(TransactionItem(i) for i in range(conts.shape[0])))#,label_cls=TransactionItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = list(range(int(n_train*.8))) + list(range(n_train, n_train + int(n_test*0.8)))\n",
    "valid_idx = list(np.setdiff1d(range(n_train+n_test), train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split_by_idxs(train_idx, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.label_from_func(func=lambda x:x,label_cls=TransactionItemList)\n",
    "#data = data.label_from_lists(df[['isFraud', 'isTest']].values[train_idx], df[['isFraud', 'isTest']].values[valid_idx])\n",
    "data = data.label_from_lists(df['isTest'].values[train_idx], df['isTest'].values[valid_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.add_test(ItemList(items=(TransactionItem(i) for i in range(n_train+n_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBunch;\n",
       "\n",
       "Train: LabelList (877784 items)\n",
       "x: ItemList\n",
       "0,1,2,3,4\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (219447 items)\n",
       "x: ItemList\n",
       "472432,472433,472434,472435,472436\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (1097231 items)\n",
       "x: ItemList\n",
       "0,1,2,3,4\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat:int)->int: return min(50, (n_cat//2)+1)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class TransactionAutoencoder(Module):\n",
    "    def __init__(self, cat_names, cont_names,encoder_layers,decoder_layers=None):\n",
    "        self.cat_names, self.cont_names = cat_names, cont_names\n",
    "        self.cats_nunique  = cats.max(dim=0)[0]+1\n",
    "        self.cat_embs = nn.ModuleList(\n",
    "            [nn.Embedding(cat_nunique,emb_sz_rule(cat_nunique)) for cat_nunique in self.cats_nunique])\n",
    "        self.emb_total_dim = sum([emb_sz_rule(cat_nunique) for cat_nunique in self.cats_nunique])\n",
    "        self.cont_total_dim = conts.shape[1]\n",
    "        decoder_layers=ifnone(decoder_layers, encoder_layers[::-1])\n",
    "        encoder_layers.insert( 0,int(self.cont_total_dim + self.emb_total_dim))\n",
    "        decoder_layers.append(int(self.cont_total_dim + self.cats_nunique.sum()))\n",
    "        self.encoder = nn.Sequential(\n",
    "            *flatten([[nn.Linear(encoder_layers[i],encoder_layers[i+1]),nn.ReLU(), nn.BatchNorm1d(encoder_layers[i+1])]\n",
    "              for i in range(len(encoder_layers)-1)])[:])\n",
    "        self.decoder = nn.Sequential(\n",
    "            *flatten([[nn.Linear(decoder_layers[i],decoder_layers[i+1]),nn.ReLU(), nn.BatchNorm1d(decoder_layers[i+1])]\n",
    "              for i in range(len(decoder_layers)-1)])[:-2])\n",
    "        \n",
    "        \n",
    "        self.is_test = nn.Sequential(\n",
    "            *flatten([[nn.Linear(int(2**i),int(2**(i-1))),nn.ReLU(), nn.BatchNorm1d(int(2**(i-1)))]\n",
    "              for i in range(int(np.log2(encoder_layers[-1])),2,-1)]), nn.Linear(4,2))\n",
    "\n",
    "    def forward(self,x_cats,x_conts):\n",
    "        _x_cats = [x_cats[:,i] for i in range(x_cats.shape[1])]\n",
    "        x = torch.cat([cat_emb(cat) for cat_emb,cat in zip(self.cat_embs, _x_cats)], dim=-1)\n",
    "        x = torch.cat([x, x_conts], dim=-1)\n",
    "        latent = self.encoder(x)\n",
    "        x = self.decoder(latent)\n",
    "        is_test = self.is_test(latent)\n",
    "        return x_cats,x_conts, x[:,:self.cats_nunique.sum()],x[:,self.cats_nunique.sum():],is_test,latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReconstructionLoss(Module):\n",
    "    def __init__(self, cats_nunique):\n",
    "        self.cats_nunique = cats_nunique\n",
    "\n",
    "    def forward(self, from_forward, t_is_test):\n",
    "        t_cats, t_conts, p_cats, p_conts, p_is_test, p_latent = from_forward\n",
    "        cat_loss = 0\n",
    "        s_hot = e_hot = 0\n",
    "        for i, n_hot in enumerate(self.cats_nunique):\n",
    "            e_hot = s_hot + n_hot\n",
    "            cat_loss += nn.CrossEntropyLoss()(p_cats[:,s_hot:e_hot], t_cats[:,i])\n",
    "            s_hot = e_hot\n",
    "        cat_loss /= i\n",
    "        cont_loss = nn.MSELoss()(p_conts, t_conts)\n",
    "        adversarial_loss = nn.CrossEntropyLoss()(p_is_test, t_is_test)\n",
    "        return 30*cat_loss + cont_loss #+ adversarial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_latent = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "encoder_layers=[512,256,128,d_latent]\n",
    "decoder_layers=encoder_layers[::-1]\n",
    "net = TransactionAutoencoder(cat_names, cont_names,encoder_layers,decoder_layers)\n",
    "learner = Learner(data,net,loss_func=ReconstructionLoss(net.cats_nunique),callback_fns=ShowGraph).to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=DataBunch;\n",
       "\n",
       "Train: LabelList (877784 items)\n",
       "x: ItemList\n",
       "0,1,2,3,4\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (219447 items)\n",
       "x: ItemList\n",
       "472432,472433,472434,472435,472436\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (1097231 items)\n",
       "x: ItemList\n",
       "0,1,2,3,4\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=TransactionAutoencoder(\n",
       "  (cat_embs): ModuleList(\n",
       "    (0): Embedding(7, 4)\n",
       "    (1): Embedding(4, 3)\n",
       "    (2): Embedding(3, 2)\n",
       "    (3): Embedding(3, 2)\n",
       "    (4): Embedding(3, 2)\n",
       "    (5): Embedding(4, 3)\n",
       "    (6): Embedding(3, 2)\n",
       "    (7): Embedding(3, 2)\n",
       "    (8): Embedding(3, 2)\n",
       "    (9): Embedding(3, 2)\n",
       "    (10): Embedding(3, 2)\n",
       "    (11): Embedding(9, 5)\n",
       "    (12): Embedding(5, 3)\n",
       "    (13): Embedding(6, 4)\n",
       "    (14): Embedding(4, 3)\n",
       "    (15): Embedding(3, 2)\n",
       "    (16): Embedding(3, 2)\n",
       "    (17): Embedding(5, 3)\n",
       "    (18): Embedding(4, 3)\n",
       "    (19): Embedding(3, 2)\n",
       "    (20): Embedding(3, 2)\n",
       "    (21): Embedding(4, 3)\n",
       "    (22): Embedding(4, 3)\n",
       "    (23): Embedding(5, 3)\n",
       "    (24): Embedding(8, 5)\n",
       "    (25): Embedding(6, 4)\n",
       "    (26): Embedding(5, 3)\n",
       "    (27): Embedding(4, 3)\n",
       "    (28): Embedding(3, 2)\n",
       "    (29): Embedding(4, 3)\n",
       "    (30): Embedding(4, 3)\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=1319, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=512, out_features=1363, bias=True)\n",
       "  )\n",
       "  (is_test): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=ReconstructionLoss(), metrics=[], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class 'fastai.train.ShowGraph'>], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Embedding(7, 4)\n",
       "  (1): Embedding(4, 3)\n",
       "  (2): Embedding(3, 2)\n",
       "  (3): Embedding(3, 2)\n",
       "  (4): Embedding(3, 2)\n",
       "  (5): Embedding(4, 3)\n",
       "  (6): Embedding(3, 2)\n",
       "  (7): Embedding(3, 2)\n",
       "  (8): Embedding(3, 2)\n",
       "  (9): Embedding(3, 2)\n",
       "  (10): Embedding(3, 2)\n",
       "  (11): Embedding(9, 5)\n",
       "  (12): Embedding(5, 3)\n",
       "  (13): Embedding(6, 4)\n",
       "  (14): Embedding(4, 3)\n",
       "  (15): Embedding(3, 2)\n",
       "  (16): Embedding(3, 2)\n",
       "  (17): Embedding(5, 3)\n",
       "  (18): Embedding(4, 3)\n",
       "  (19): Embedding(3, 2)\n",
       "  (20): Embedding(3, 2)\n",
       "  (21): Embedding(4, 3)\n",
       "  (22): Embedding(4, 3)\n",
       "  (23): Embedding(5, 3)\n",
       "  (24): Embedding(8, 5)\n",
       "  (25): Embedding(6, 4)\n",
       "  (26): Embedding(5, 3)\n",
       "  (27): Embedding(4, 3)\n",
       "  (28): Embedding(3, 2)\n",
       "  (29): Embedding(4, 3)\n",
       "  (30): Embedding(4, 3)\n",
       "  (31): Linear(in_features=1319, out_features=512, bias=True)\n",
       "  (32): ReLU()\n",
       "  (33): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (35): ReLU()\n",
       "  (36): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (37): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (38): ReLU()\n",
       "  (39): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (40): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (41): ReLU()\n",
       "  (42): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (43): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (44): ReLU()\n",
       "  (45): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (46): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (47): ReLU()\n",
       "  (48): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (49): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (50): ReLU()\n",
       "  (51): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (52): Linear(in_features=512, out_features=1363, bias=True)\n",
       "  (53): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (54): ReLU()\n",
       "  (55): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (56): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (57): ReLU()\n",
       "  (58): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (59): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (60): ReLU()\n",
       "  (61): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (62): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (63): ReLU()\n",
       "  (64): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (65): Linear(in_features=4, out_features=2, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionAutoencoder\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Embedding            [4]                  28         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [5]                  45         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  15         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [4]                  24         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  15         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  15         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [5]                  40         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [4]                  24         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  15         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [2]                  6          True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Embedding            [3]                  12         True      \n",
       "______________________________________________________________________\n",
       "Linear               [512]                675,840    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [512]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [512]                1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [256]                131,328    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [256]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [256]                512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [128]                32,896     True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [128]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [128]                256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [64]                 8,256      True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64]                 0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [64]                 128        True      \n",
       "______________________________________________________________________\n",
       "Linear               [128]                8,320      True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [128]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [128]                256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [256]                33,024     True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [256]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [256]                512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [512]                131,584    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [512]                0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [512]                1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [1363]               699,219    True      \n",
       "______________________________________________________________________\n",
       "Linear               [32]                 2,080      True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [32]                 0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [32]                 64         True      \n",
       "______________________________________________________________________\n",
       "Linear               [16]                 528        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [16]                 0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [16]                 32         True      \n",
       "______________________________________________________________________\n",
       "Linear               [8]                  136        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [8]                  0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [8]                  16         True      \n",
       "______________________________________________________________________\n",
       "Linear               [4]                  36         True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [4]                  0          False     \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [4]                  8          True      \n",
       "______________________________________________________________________\n",
       "Linear               [2]                  10         True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 1,727,496\n",
       "Total trainable params: 1,727,496\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : ReconstructionLoss\n",
       "======================================================================\n",
       "Callbacks functions applied "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionAutoencoder(\n",
       "  (cat_embs): ModuleList(\n",
       "    (0): Embedding(7, 4)\n",
       "    (1): Embedding(4, 3)\n",
       "    (2): Embedding(3, 2)\n",
       "    (3): Embedding(3, 2)\n",
       "    (4): Embedding(3, 2)\n",
       "    (5): Embedding(4, 3)\n",
       "    (6): Embedding(3, 2)\n",
       "    (7): Embedding(3, 2)\n",
       "    (8): Embedding(3, 2)\n",
       "    (9): Embedding(3, 2)\n",
       "    (10): Embedding(3, 2)\n",
       "    (11): Embedding(9, 5)\n",
       "    (12): Embedding(5, 3)\n",
       "    (13): Embedding(6, 4)\n",
       "    (14): Embedding(4, 3)\n",
       "    (15): Embedding(3, 2)\n",
       "    (16): Embedding(3, 2)\n",
       "    (17): Embedding(5, 3)\n",
       "    (18): Embedding(4, 3)\n",
       "    (19): Embedding(3, 2)\n",
       "    (20): Embedding(3, 2)\n",
       "    (21): Embedding(4, 3)\n",
       "    (22): Embedding(4, 3)\n",
       "    (23): Embedding(5, 3)\n",
       "    (24): Embedding(8, 5)\n",
       "    (25): Embedding(6, 4)\n",
       "    (26): Embedding(5, 3)\n",
       "    (27): Embedding(4, 3)\n",
       "    (28): Embedding(3, 2)\n",
       "    (29): Embedding(4, 3)\n",
       "    (30): Embedding(4, 3)\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=1319, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=512, out_features=1363, bias=True)\n",
       "  )\n",
       "  (is_test): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.batch_size = 4096*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder_fname = 'autoencoder_loss0.0008val0.0010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'autoencoder_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "loaded = False\n",
    "try:\n",
    "    learner.load(autoencoder_fname, strict=False)\n",
    "    loaded = True\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.74E-03\n",
      "Min loss divided by 10: 6.92E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not loaded:\n",
    "    learner.lr_find()\n",
    "    learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>35.946514</td>\n",
       "      <td>29.293501</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.184576</td>\n",
       "      <td>12.820310</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15.225582</td>\n",
       "      <td>7.752847</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.076923</td>\n",
       "      <td>5.155647</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.741526</td>\n",
       "      <td>3.197262</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.473769</td>\n",
       "      <td>2.052161</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.987906</td>\n",
       "      <td>1.484996</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.025873</td>\n",
       "      <td>1.257626</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.404331</td>\n",
       "      <td>1.088914</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.003986</td>\n",
       "      <td>0.948122</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.744149</td>\n",
       "      <td>0.950372</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.572704</td>\n",
       "      <td>0.866774</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.458107</td>\n",
       "      <td>0.871464</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.379716</td>\n",
       "      <td>0.813105</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.328980</td>\n",
       "      <td>0.818788</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.291871</td>\n",
       "      <td>0.792147</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.266696</td>\n",
       "      <td>0.813932</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.249483</td>\n",
       "      <td>0.805732</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.237982</td>\n",
       "      <td>0.793802</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.230203</td>\n",
       "      <td>0.790755</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not loaded: learner.fit_one_cycle(20,2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907553"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = learner.validate()[0]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not loaded:\n",
    "    try:\n",
    "        autoencoder_fname = f'autoencoder_loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "    except:\n",
    "        autoencoder_fname = f'autoencoder_lossval{val:.04f}'\n",
    "    learner.save(autoencoder_fname)\n",
    "    print(autoencoder_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.fix_dl.batch_size = data.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='34' class='' max='34', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [34/34 00:10<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_latents(ds_type):\n",
    "    latents = torch.empty(0,d_latent)\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(ds_type)), total=len(learner.dl(ds_type))):\n",
    "            t_cats, t_conts, p_cats, p_conts, p_is_test, p_latent = learner.pred_batch(ds_type=ds_type, batch=batch)\n",
    "            latents = torch.cat([latents, p_latent], dim = 0)\n",
    "    return latents\n",
    "            \n",
    "all_latents  = get_latents(DatasetType.Test)\n",
    "latents      = all_latents[:n_train]\n",
    "latents_test = all_latents[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([590540, 64])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590540"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([590540, 64])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_latent = pd.DataFrame({**{'isFraud': df['isFraud'][:n_train].values.astype(np.int32)},\n",
    "                          **{f'l_{i}' : latents[:,i].cpu().numpy().T for i in range(d_latent)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_latent_test = pd.DataFrame({**{f'l_{i}' : latents_test[:,i].cpu().numpy().T for i in range(d_latent)}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = range(int(n_train*.8))\n",
    "valid_idx = list(np.setdiff1d(range(n_train), train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "472427    0\n",
       "472428    0\n",
       "472429    0\n",
       "472430    0\n",
       "472431    0\n",
       "Name: isFraud, Length: 472432, dtype: int32"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latent.iloc[train_idx]['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_0</th>\n",
       "      <th>l_1</th>\n",
       "      <th>l_2</th>\n",
       "      <th>l_3</th>\n",
       "      <th>l_4</th>\n",
       "      <th>l_5</th>\n",
       "      <th>l_6</th>\n",
       "      <th>l_7</th>\n",
       "      <th>l_8</th>\n",
       "      <th>l_9</th>\n",
       "      <th>...</th>\n",
       "      <th>l_54</th>\n",
       "      <th>l_55</th>\n",
       "      <th>l_56</th>\n",
       "      <th>l_57</th>\n",
       "      <th>l_58</th>\n",
       "      <th>l_59</th>\n",
       "      <th>l_60</th>\n",
       "      <th>l_61</th>\n",
       "      <th>l_62</th>\n",
       "      <th>l_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.609179</td>\n",
       "      <td>-0.195527</td>\n",
       "      <td>-0.105292</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.309707</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.391171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>2.290798</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.133883</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.536446</td>\n",
       "      <td>-0.056623</td>\n",
       "      <td>1.901735</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.516381</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.463647</td>\n",
       "      <td>-0.920896</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>2.099545</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.743241</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.087067</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.255664</td>\n",
       "      <td>-0.621156</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.335764</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.913942</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.247482</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.419852</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.671915</td>\n",
       "      <td>-0.058674</td>\n",
       "      <td>1.915679</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.166618</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.836539</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>2.201424</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.908551</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>0.788680</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.213443</td>\n",
       "      <td>3.273848</td>\n",
       "      <td>1.116109</td>\n",
       "      <td>2.519453</td>\n",
       "      <td>1.404840</td>\n",
       "      <td>1.413893</td>\n",
       "      <td>1.080059</td>\n",
       "      <td>...</td>\n",
       "      <td>1.185768</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.389232</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.345746</td>\n",
       "      <td>0.499470</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.436681</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.074411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.495652</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.207343</td>\n",
       "      <td>-0.124450</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.338953</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.039480</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.228217</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.059690</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.237800</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.353540</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.350993</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.004325</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.018839</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.387507</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.439003</td>\n",
       "      <td>-0.278593</td>\n",
       "      <td>1.590646</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.049899</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.164416</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>2.144663</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.935095</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.577858</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>0.581762</td>\n",
       "      <td>3.125286</td>\n",
       "      <td>0.447913</td>\n",
       "      <td>2.764018</td>\n",
       "      <td>0.826874</td>\n",
       "      <td>0.693522</td>\n",
       "      <td>1.689709</td>\n",
       "      <td>...</td>\n",
       "      <td>1.288719</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.555423</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.771149</td>\n",
       "      <td>0.467356</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.396906</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.796146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.230840</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.262895</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.677134</td>\n",
       "      <td>-0.333708</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.089756</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.334646</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.470194</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.617859</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.569100</td>\n",
       "      <td>0.488256</td>\n",
       "      <td>0.876891</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>0.582945</td>\n",
       "      <td>0.972551</td>\n",
       "      <td>2.139534</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>0.229470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733332</td>\n",
       "      <td>3.211989</td>\n",
       "      <td>2.160134</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.064544</td>\n",
       "      <td>1.795471</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.848765</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.519405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>2.030429</td>\n",
       "      <td>0.519201</td>\n",
       "      <td>0.529962</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>1.725346</td>\n",
       "      <td>0.779279</td>\n",
       "      <td>1.956388</td>\n",
       "      <td>0.382845</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347772</td>\n",
       "      <td>1.982453</td>\n",
       "      <td>1.495198</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.422010</td>\n",
       "      <td>1.487832</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>2.106795</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>0.478891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.260367</td>\n",
       "      <td>-0.346827</td>\n",
       "      <td>1.522379</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.018828</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.270137</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.625112</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.298443</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.357821</td>\n",
       "      <td>-0.371919</td>\n",
       "      <td>1.599710</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.164648</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.626206</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.325426</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.607308</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.332947</td>\n",
       "      <td>-0.324936</td>\n",
       "      <td>1.462072</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.030041</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.854380</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.911080</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.675793</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.024762</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>0.110635</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.102350</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>1.387746</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.225757</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.765109</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.263309</td>\n",
       "      <td>2.848909</td>\n",
       "      <td>1.336568</td>\n",
       "      <td>1.125064</td>\n",
       "      <td>-0.319813</td>\n",
       "      <td>0.492660</td>\n",
       "      <td>1.467853</td>\n",
       "      <td>...</td>\n",
       "      <td>1.559010</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>0.872801</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.269558</td>\n",
       "      <td>0.741988</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.676543</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.318365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.242571</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>0.377586</td>\n",
       "      <td>1.665001</td>\n",
       "      <td>1.931354</td>\n",
       "      <td>1.396534</td>\n",
       "      <td>1.063168</td>\n",
       "      <td>1.028612</td>\n",
       "      <td>1.956410</td>\n",
       "      <td>...</td>\n",
       "      <td>1.776819</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.973838</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.619601</td>\n",
       "      <td>0.291847</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>0.927611</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>2.695443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.218392</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>0.531952</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.056003</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.223250</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.035515</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.173434</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.191898</td>\n",
       "      <td>-0.452320</td>\n",
       "      <td>1.191781</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.305012</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.055068</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.250139</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.021318</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.446961</td>\n",
       "      <td>-0.012607</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.134272</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.292654</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.040672</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.320826</td>\n",
       "      <td>-0.101441</td>\n",
       "      <td>1.755040</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.832971</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.145776</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.738213</td>\n",
       "      <td>0.420359</td>\n",
       "      <td>-0.319265</td>\n",
       "      <td>1.675901</td>\n",
       "      <td>0.741984</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>1.180628</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561747</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.577723</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.698479</td>\n",
       "      <td>0.153115</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.367787</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>0.348536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.421623</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.365476</td>\n",
       "      <td>-0.187582</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.399220</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.179672</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.216799</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.341360</td>\n",
       "      <td>-0.410244</td>\n",
       "      <td>1.618327</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.027599</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.137960</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.440955</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.241761</td>\n",
       "      <td>-0.262673</td>\n",
       "      <td>1.884794</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.309300</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.887875</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.461098</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.461666</td>\n",
       "      <td>-0.177301</td>\n",
       "      <td>1.687861</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.037513</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.020931</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>2.227655</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>1.175505</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.488396</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>0.135089</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.187098</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.469647</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.426990</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.053354</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.189331</td>\n",
       "      <td>-0.160792</td>\n",
       "      <td>1.267263</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.063240</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.868536</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.868665</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.138799</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.389943</td>\n",
       "      <td>-0.452844</td>\n",
       "      <td>1.514426</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.295108</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.480760</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.924750</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.889535</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472402</th>\n",
       "      <td>0.750305</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.031886</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.372380</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>-0.120512</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.040534</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.291813</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472403</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>3.558333</td>\n",
       "      <td>1.059764</td>\n",
       "      <td>0.957232</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>2.434363</td>\n",
       "      <td>0.491920</td>\n",
       "      <td>1.915502</td>\n",
       "      <td>1.970494</td>\n",
       "      <td>1.059834</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421243</td>\n",
       "      <td>1.940750</td>\n",
       "      <td>1.423151</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.421295</td>\n",
       "      <td>1.766112</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.972118</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>2.799652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>0.832666</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>0.069613</td>\n",
       "      <td>-0.536494</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>0.082592</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.969579</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.955106</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.929051</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472405</th>\n",
       "      <td>0.755288</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.044386</td>\n",
       "      <td>-0.491586</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.501300</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.405362</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.356849</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.024637</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472406</th>\n",
       "      <td>1.743782</td>\n",
       "      <td>-0.329445</td>\n",
       "      <td>-0.358039</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.765631</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.183632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.474283</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>1.837311</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.496649</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.331769</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472407</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>2.816470</td>\n",
       "      <td>1.511683</td>\n",
       "      <td>1.628064</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>1.079511</td>\n",
       "      <td>0.451910</td>\n",
       "      <td>2.499885</td>\n",
       "      <td>1.355315</td>\n",
       "      <td>0.405771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766408</td>\n",
       "      <td>2.619166</td>\n",
       "      <td>1.717000</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.552534</td>\n",
       "      <td>2.053025</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>2.459566</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.494805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472408</th>\n",
       "      <td>1.278050</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.573515</td>\n",
       "      <td>-0.300784</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.390955</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.032632</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.340274</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.529846</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472409</th>\n",
       "      <td>0.640653</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.357693</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.182531</td>\n",
       "      <td>-0.679101</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.511426</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.192422</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.046475</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.291977</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472410</th>\n",
       "      <td>0.895878</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-0.098834</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.294456</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.193298</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.538992</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.469915</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472411</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.511964</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>2.797644</td>\n",
       "      <td>2.652290</td>\n",
       "      <td>1.722087</td>\n",
       "      <td>2.118685</td>\n",
       "      <td>2.169930</td>\n",
       "      <td>2.546611</td>\n",
       "      <td>3.290248</td>\n",
       "      <td>...</td>\n",
       "      <td>2.986799</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>3.247229</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.970740</td>\n",
       "      <td>2.528862</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.308565</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>3.573133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472412</th>\n",
       "      <td>-0.037095</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.185792</td>\n",
       "      <td>0.237991</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.298904</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.265615</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.333892</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472413</th>\n",
       "      <td>0.577747</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.461695</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.598840</td>\n",
       "      <td>-0.386424</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.097672</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.341340</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.094251</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.556892</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472414</th>\n",
       "      <td>0.919080</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.566784</td>\n",
       "      <td>0.048972</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.045147</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.211983</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.685091</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.353286</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472415</th>\n",
       "      <td>0.570689</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.189967</td>\n",
       "      <td>-0.068764</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.309328</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.042674</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.154286</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472416</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>2.763376</td>\n",
       "      <td>1.608359</td>\n",
       "      <td>1.675724</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>0.998575</td>\n",
       "      <td>0.424911</td>\n",
       "      <td>2.552268</td>\n",
       "      <td>1.269643</td>\n",
       "      <td>0.367386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769324</td>\n",
       "      <td>2.687429</td>\n",
       "      <td>1.794389</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.620850</td>\n",
       "      <td>2.201414</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>2.551684</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.456011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472417</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.148374</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>2.022756</td>\n",
       "      <td>2.421131</td>\n",
       "      <td>1.263120</td>\n",
       "      <td>2.353860</td>\n",
       "      <td>1.321656</td>\n",
       "      <td>3.693033</td>\n",
       "      <td>1.185097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125833</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.166990</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>2.354766</td>\n",
       "      <td>1.334737</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.798811</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>2.568419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472418</th>\n",
       "      <td>-0.727812</td>\n",
       "      <td>-0.066251</td>\n",
       "      <td>1.827588</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.078177</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.724116</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.535611</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472419</th>\n",
       "      <td>0.972927</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.118444</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.526872</td>\n",
       "      <td>-0.030163</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>0.119884</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.225904</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.090316</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.408434</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472420</th>\n",
       "      <td>0.093553</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.151448</td>\n",
       "      <td>0.210801</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.124453</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.191626</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.540724</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.255783</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472421</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.661041</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.905523</td>\n",
       "      <td>0.953712</td>\n",
       "      <td>0.649869</td>\n",
       "      <td>1.822134</td>\n",
       "      <td>0.404249</td>\n",
       "      <td>1.511329</td>\n",
       "      <td>1.315282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.453341</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.444492</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.582221</td>\n",
       "      <td>-0.108083</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.940321</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.022372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472422</th>\n",
       "      <td>0.618341</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.138418</td>\n",
       "      <td>-0.074806</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.287503</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.085022</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>-0.135503</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.356228</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472423</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.301564</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.995762</td>\n",
       "      <td>0.600433</td>\n",
       "      <td>0.495310</td>\n",
       "      <td>1.989642</td>\n",
       "      <td>0.589595</td>\n",
       "      <td>1.553187</td>\n",
       "      <td>1.134119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571749</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.641506</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.653707</td>\n",
       "      <td>0.382427</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>2.056797</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.266557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472424</th>\n",
       "      <td>-0.287568</td>\n",
       "      <td>-0.075078</td>\n",
       "      <td>1.496713</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-1.138371</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>1.019654</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.667454</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>1.924935</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.989357</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472425</th>\n",
       "      <td>0.156569</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.411211</td>\n",
       "      <td>-0.222947</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.171472</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.492166</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.486175</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>-0.305479</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472426</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.677489</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.928416</td>\n",
       "      <td>0.960658</td>\n",
       "      <td>0.655031</td>\n",
       "      <td>1.816348</td>\n",
       "      <td>0.381445</td>\n",
       "      <td>1.529887</td>\n",
       "      <td>1.328999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.479734</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.451886</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.602086</td>\n",
       "      <td>-0.070910</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.944310</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472427</th>\n",
       "      <td>-0.431395</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.283861</td>\n",
       "      <td>0.327021</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.426984</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>0.131168</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.239545</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472428</th>\n",
       "      <td>0.931880</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-0.067158</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.307973</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.101781</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.573610</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.410653</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472429</th>\n",
       "      <td>-1.187679</td>\n",
       "      <td>1.419637</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>1.944800</td>\n",
       "      <td>2.500134</td>\n",
       "      <td>1.038277</td>\n",
       "      <td>2.262719</td>\n",
       "      <td>0.202585</td>\n",
       "      <td>0.521546</td>\n",
       "      <td>2.109949</td>\n",
       "      <td>...</td>\n",
       "      <td>2.167187</td>\n",
       "      <td>-0.749603</td>\n",
       "      <td>1.436872</td>\n",
       "      <td>-1.098345</td>\n",
       "      <td>1.010410</td>\n",
       "      <td>0.937264</td>\n",
       "      <td>-1.232385</td>\n",
       "      <td>1.247445</td>\n",
       "      <td>-0.840590</td>\n",
       "      <td>1.614556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472430</th>\n",
       "      <td>1.188356</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.756810</td>\n",
       "      <td>-0.321901</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.189642</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.331408</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.106951</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.566980</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472431</th>\n",
       "      <td>0.435206</td>\n",
       "      <td>-0.690702</td>\n",
       "      <td>-0.464052</td>\n",
       "      <td>-0.611569</td>\n",
       "      <td>-0.826007</td>\n",
       "      <td>-0.305919</td>\n",
       "      <td>-0.464352</td>\n",
       "      <td>-0.560705</td>\n",
       "      <td>-0.644472</td>\n",
       "      <td>-0.514663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509150</td>\n",
       "      <td>-0.191646</td>\n",
       "      <td>-0.513325</td>\n",
       "      <td>-0.060041</td>\n",
       "      <td>-0.500395</td>\n",
       "      <td>-0.592811</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>-0.618048</td>\n",
       "      <td>0.228846</td>\n",
       "      <td>-0.556857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>472432 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             l_0       l_1       l_2       l_3       l_4       l_5       l_6  \\\n",
       "0       1.609179 -0.195527 -0.105292 -0.611569 -0.309707 -1.138371 -0.464352   \n",
       "1       0.536446 -0.056623  1.901735 -0.611569 -0.826007 -1.138371 -0.464352   \n",
       "2      -0.087067 -0.690702 -0.464052 -0.611569 -0.255664 -0.621156 -0.464352   \n",
       "3       0.671915 -0.058674  1.915679 -0.611569 -0.826007 -1.138371 -0.464352   \n",
       "4      -1.187679  0.788680 -0.464052  1.213443  3.273848  1.116109  2.519453   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "472427 -0.431395 -0.690702 -0.464052 -0.611569 -0.283861  0.327021 -0.464352   \n",
       "472428  0.931880 -0.690702 -0.464052 -0.611569 -0.826007 -0.067158 -0.464352   \n",
       "472429 -1.187679  1.419637 -0.464052  1.944800  2.500134  1.038277  2.262719   \n",
       "472430  1.188356 -0.690702 -0.464052 -0.611569 -0.756810 -0.321901 -0.464352   \n",
       "472431  0.435206 -0.690702 -0.464052 -0.611569 -0.826007 -0.305919 -0.464352   \n",
       "\n",
       "             l_7       l_8       l_9  ...      l_54      l_55      l_56  \\\n",
       "0      -0.560705 -0.644472 -0.391171  ... -0.509150 -0.749603 -0.513325   \n",
       "1      -0.560705  1.516381 -0.514663  ... -0.509150 -0.749603 -0.463647   \n",
       "2      -0.560705 -0.644472 -0.514663  ... -0.509150 -0.335764 -0.513325   \n",
       "3      -0.560705  1.166618 -0.514663  ... -0.509150 -0.749603 -0.513325   \n",
       "4       1.404840  1.413893  1.080059  ...  1.185768 -0.749603  1.389232   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "472427 -0.560705 -0.644472 -0.514663  ... -0.509150 -0.426984 -0.513325   \n",
       "472428 -0.560705 -0.644472 -0.514663  ... -0.509150 -0.307973 -0.513325   \n",
       "472429  0.202585  0.521546  2.109949  ...  2.167187 -0.749603  1.436872   \n",
       "472430 -0.560705 -0.644472 -0.514663  ... -0.509150 -0.189642 -0.513325   \n",
       "472431 -0.560705 -0.644472 -0.514663  ... -0.509150 -0.191646 -0.513325   \n",
       "\n",
       "            l_57      l_58      l_59      l_60      l_61      l_62      l_63  \n",
       "0       2.290798 -0.500395 -0.592811  1.133883 -0.618048  0.014759 -0.556857  \n",
       "1      -0.920896 -0.500395 -0.592811  2.099545 -0.618048  0.743241 -0.556857  \n",
       "2       0.913942 -0.500395 -0.592811  0.247482 -0.618048  0.419852 -0.556857  \n",
       "3      -0.836539 -0.500395 -0.592811  2.201424 -0.618048  0.908551 -0.556857  \n",
       "4      -1.098345  2.345746  0.499470 -1.232385  1.436681 -0.840590  1.074411  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "472427  0.131168 -0.500395 -0.592811  0.011408 -0.618048  0.239545 -0.556857  \n",
       "472428 -0.101781 -0.500395 -0.592811  0.573610 -0.618048  0.410653 -0.556857  \n",
       "472429 -1.098345  1.010410  0.937264 -1.232385  1.247445 -0.840590  1.614556  \n",
       "472430 -0.331408 -0.500395 -0.592811  0.106951 -0.618048  0.566980 -0.556857  \n",
       "472431 -0.060041 -0.500395 -0.592811  0.019629 -0.618048  0.228846 -0.556857  \n",
       "\n",
       "[472432 rows x 64 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latent.loc[train_idx,df_latent.columns != 'isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data=lgb.Dataset(df_latent.loc[train_idx,df_latent.columns!='isFraud'], label=df_latent.iloc[train_idx]['isFraud'])\n",
    "val_data=lgb.Dataset(df_latent.loc[valid_idx,df_latent.columns!='isFraud'], label=df_latent.iloc[valid_idx]['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'D1',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'TransactionAmt',\n",
       " 'TransactionAmt_decimals',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V11',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V12',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V13',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V14',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V15',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V16',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V17',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V18',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V19',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V2',\n",
       " 'V20',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V21',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V22',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V23',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V24',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V25',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V26',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V27',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V28',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V29',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V3',\n",
       " 'V30',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V31',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V32',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V33',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V4',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V5',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V6',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V7',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V8',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V9',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'card1',\n",
       " 'card1_count_full',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card5',\n",
       " 'day',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'dow',\n",
       " 'hour',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_32',\n",
       " 'DeviceInfo_freq',\n",
       " 'DeviceType_freq',\n",
       " 'M1_freq',\n",
       " 'M2_freq',\n",
       " 'M3_freq',\n",
       " 'M4_freq',\n",
       " 'M5_freq',\n",
       " 'M6_freq',\n",
       " 'M7_freq',\n",
       " 'M8_freq',\n",
       " 'M9_freq',\n",
       " 'P_emaildomain_freq',\n",
       " 'ProductCD_freq',\n",
       " 'R_emaildomain_freq',\n",
       " 'card4_freq',\n",
       " 'card6_freq',\n",
       " 'id_12_freq',\n",
       " 'id_15_freq',\n",
       " 'id_16_freq',\n",
       " 'id_23_freq',\n",
       " 'id_27_freq',\n",
       " 'id_28_freq',\n",
       " 'id_29_freq',\n",
       " 'id_30_freq',\n",
       " 'id_31_freq',\n",
       " 'id_33_freq',\n",
       " 'id_34_freq',\n",
       " 'id_35_freq',\n",
       " 'id_36_freq',\n",
       " 'id_37_freq',\n",
       " 'id_38_freq',\n",
       " 'C1_na',\n",
       " 'C10_na',\n",
       " 'C11_na',\n",
       " 'C12_na',\n",
       " 'C13_na',\n",
       " 'C14_na',\n",
       " 'C2_na',\n",
       " 'C3_na',\n",
       " 'C4_na',\n",
       " 'C5_na',\n",
       " 'C6_na',\n",
       " 'C7_na',\n",
       " 'C8_na',\n",
       " 'C9_na',\n",
       " 'D1_na',\n",
       " 'D10_na',\n",
       " 'D11_na',\n",
       " 'D12_na',\n",
       " 'D13_na',\n",
       " 'D14_na',\n",
       " 'D15_na',\n",
       " 'D2_na',\n",
       " 'D3_na',\n",
       " 'D4_na',\n",
       " 'D5_na',\n",
       " 'D6_na',\n",
       " 'D7_na',\n",
       " 'D8_na',\n",
       " 'D9_na',\n",
       " 'V1_na',\n",
       " 'V10_na',\n",
       " 'V100_na',\n",
       " 'V101_na',\n",
       " 'V102_na',\n",
       " 'V103_na',\n",
       " 'V104_na',\n",
       " 'V105_na',\n",
       " 'V106_na',\n",
       " 'V107_na',\n",
       " 'V108_na',\n",
       " 'V109_na',\n",
       " 'V11_na',\n",
       " 'V110_na',\n",
       " 'V111_na',\n",
       " 'V112_na',\n",
       " 'V113_na',\n",
       " 'V114_na',\n",
       " 'V115_na',\n",
       " 'V116_na',\n",
       " 'V117_na',\n",
       " 'V118_na',\n",
       " 'V119_na',\n",
       " 'V12_na',\n",
       " 'V120_na',\n",
       " 'V121_na',\n",
       " 'V122_na',\n",
       " 'V123_na',\n",
       " 'V124_na',\n",
       " 'V125_na',\n",
       " 'V126_na',\n",
       " 'V127_na',\n",
       " 'V128_na',\n",
       " 'V129_na',\n",
       " 'V13_na',\n",
       " 'V130_na',\n",
       " 'V131_na',\n",
       " 'V132_na',\n",
       " 'V133_na',\n",
       " 'V134_na',\n",
       " 'V135_na',\n",
       " 'V136_na',\n",
       " 'V137_na',\n",
       " 'V138_na',\n",
       " 'V139_na',\n",
       " 'V14_na',\n",
       " 'V140_na',\n",
       " 'V141_na',\n",
       " 'V142_na',\n",
       " 'V143_na',\n",
       " 'V144_na',\n",
       " 'V145_na',\n",
       " 'V146_na',\n",
       " 'V147_na',\n",
       " 'V148_na',\n",
       " 'V149_na',\n",
       " 'V15_na',\n",
       " 'V150_na',\n",
       " 'V151_na',\n",
       " 'V152_na',\n",
       " 'V153_na',\n",
       " 'V154_na',\n",
       " 'V155_na',\n",
       " 'V156_na',\n",
       " 'V157_na',\n",
       " 'V158_na',\n",
       " 'V159_na',\n",
       " 'V16_na',\n",
       " 'V160_na',\n",
       " 'V161_na',\n",
       " 'V162_na',\n",
       " 'V163_na',\n",
       " 'V164_na',\n",
       " 'V165_na',\n",
       " 'V166_na',\n",
       " 'V167_na',\n",
       " 'V168_na',\n",
       " 'V169_na',\n",
       " 'V17_na',\n",
       " 'V170_na',\n",
       " 'V171_na',\n",
       " 'V172_na',\n",
       " 'V173_na',\n",
       " 'V174_na',\n",
       " 'V175_na',\n",
       " 'V176_na',\n",
       " 'V177_na',\n",
       " 'V178_na',\n",
       " 'V179_na',\n",
       " 'V18_na',\n",
       " 'V180_na',\n",
       " 'V181_na',\n",
       " 'V182_na',\n",
       " 'V183_na',\n",
       " 'V184_na',\n",
       " 'V185_na',\n",
       " 'V186_na',\n",
       " 'V187_na',\n",
       " 'V188_na',\n",
       " 'V189_na',\n",
       " 'V19_na',\n",
       " 'V190_na',\n",
       " 'V191_na',\n",
       " 'V192_na',\n",
       " 'V193_na',\n",
       " 'V194_na',\n",
       " 'V195_na',\n",
       " 'V196_na',\n",
       " 'V197_na',\n",
       " 'V198_na',\n",
       " 'V199_na',\n",
       " 'V2_na',\n",
       " 'V20_na',\n",
       " 'V200_na',\n",
       " 'V201_na',\n",
       " 'V202_na',\n",
       " 'V203_na',\n",
       " 'V204_na',\n",
       " 'V205_na',\n",
       " 'V206_na',\n",
       " 'V207_na',\n",
       " 'V208_na',\n",
       " 'V209_na',\n",
       " 'V21_na',\n",
       " 'V210_na',\n",
       " 'V211_na',\n",
       " 'V212_na',\n",
       " 'V213_na',\n",
       " 'V214_na',\n",
       " 'V215_na',\n",
       " 'V216_na',\n",
       " 'V217_na',\n",
       " 'V218_na',\n",
       " 'V219_na',\n",
       " 'V22_na',\n",
       " 'V220_na',\n",
       " 'V221_na',\n",
       " 'V222_na',\n",
       " 'V223_na',\n",
       " 'V224_na',\n",
       " 'V225_na',\n",
       " 'V226_na',\n",
       " 'V227_na',\n",
       " 'V228_na',\n",
       " 'V229_na',\n",
       " 'V23_na',\n",
       " 'V230_na',\n",
       " 'V231_na',\n",
       " 'V232_na',\n",
       " 'V233_na',\n",
       " 'V234_na',\n",
       " 'V235_na',\n",
       " 'V236_na',\n",
       " 'V237_na',\n",
       " 'V238_na',\n",
       " 'V239_na',\n",
       " 'V24_na',\n",
       " 'V240_na',\n",
       " 'V241_na',\n",
       " 'V242_na',\n",
       " 'V243_na',\n",
       " 'V244_na',\n",
       " 'V245_na',\n",
       " 'V246_na',\n",
       " 'V247_na',\n",
       " 'V248_na',\n",
       " 'V249_na',\n",
       " 'V25_na',\n",
       " 'V250_na',\n",
       " 'V251_na',\n",
       " 'V252_na',\n",
       " 'V253_na',\n",
       " 'V254_na',\n",
       " 'V255_na',\n",
       " 'V256_na',\n",
       " 'V257_na',\n",
       " 'V258_na',\n",
       " 'V259_na',\n",
       " 'V26_na',\n",
       " 'V260_na',\n",
       " 'V261_na',\n",
       " 'V262_na',\n",
       " 'V263_na',\n",
       " 'V264_na',\n",
       " 'V265_na',\n",
       " 'V266_na',\n",
       " 'V267_na',\n",
       " 'V268_na',\n",
       " 'V269_na',\n",
       " 'V27_na',\n",
       " 'V270_na',\n",
       " 'V271_na',\n",
       " 'V272_na',\n",
       " 'V273_na',\n",
       " 'V274_na',\n",
       " 'V275_na',\n",
       " 'V276_na',\n",
       " 'V277_na',\n",
       " 'V278_na',\n",
       " 'V279_na',\n",
       " 'V28_na',\n",
       " 'V280_na',\n",
       " 'V281_na',\n",
       " 'V282_na',\n",
       " 'V283_na',\n",
       " 'V284_na',\n",
       " 'V285_na',\n",
       " 'V286_na',\n",
       " 'V287_na',\n",
       " 'V288_na',\n",
       " 'V289_na',\n",
       " 'V29_na',\n",
       " 'V290_na',\n",
       " 'V291_na',\n",
       " 'V292_na',\n",
       " 'V293_na',\n",
       " 'V294_na',\n",
       " 'V295_na',\n",
       " 'V296_na',\n",
       " 'V297_na',\n",
       " 'V298_na',\n",
       " 'V299_na',\n",
       " 'V3_na',\n",
       " 'V30_na',\n",
       " 'V300_na',\n",
       " 'V301_na',\n",
       " 'V302_na',\n",
       " 'V303_na',\n",
       " 'V304_na',\n",
       " 'V305_na',\n",
       " 'V306_na',\n",
       " 'V307_na',\n",
       " 'V308_na',\n",
       " 'V309_na',\n",
       " 'V31_na',\n",
       " 'V310_na',\n",
       " 'V311_na',\n",
       " 'V312_na',\n",
       " 'V313_na',\n",
       " 'V314_na',\n",
       " 'V315_na',\n",
       " 'V316_na',\n",
       " 'V317_na',\n",
       " 'V318_na',\n",
       " 'V319_na',\n",
       " 'V32_na',\n",
       " 'V320_na',\n",
       " 'V321_na',\n",
       " 'V322_na',\n",
       " 'V323_na',\n",
       " 'V324_na',\n",
       " 'V325_na',\n",
       " 'V326_na',\n",
       " 'V327_na',\n",
       " 'V328_na',\n",
       " 'V329_na',\n",
       " 'V33_na',\n",
       " 'V330_na',\n",
       " 'V331_na',\n",
       " 'V332_na',\n",
       " 'V333_na',\n",
       " 'V334_na',\n",
       " 'V335_na',\n",
       " 'V336_na',\n",
       " 'V337_na',\n",
       " 'V338_na',\n",
       " 'V339_na',\n",
       " 'V34_na',\n",
       " 'V35_na',\n",
       " 'V36_na',\n",
       " 'V37_na',\n",
       " 'V38_na',\n",
       " 'V39_na',\n",
       " 'V4_na',\n",
       " 'V40_na',\n",
       " 'V41_na',\n",
       " 'V42_na',\n",
       " 'V43_na',\n",
       " 'V44_na',\n",
       " 'V45_na',\n",
       " 'V46_na',\n",
       " 'V47_na',\n",
       " 'V48_na',\n",
       " 'V49_na',\n",
       " 'V5_na',\n",
       " 'V50_na',\n",
       " 'V51_na',\n",
       " 'V52_na',\n",
       " 'V53_na',\n",
       " 'V54_na',\n",
       " 'V55_na',\n",
       " 'V56_na',\n",
       " 'V57_na',\n",
       " 'V58_na',\n",
       " 'V59_na',\n",
       " 'V6_na',\n",
       " 'V60_na',\n",
       " 'V61_na',\n",
       " 'V62_na',\n",
       " 'V63_na',\n",
       " 'V64_na',\n",
       " 'V65_na',\n",
       " 'V66_na',\n",
       " 'V67_na',\n",
       " 'V68_na',\n",
       " 'V69_na',\n",
       " 'V7_na',\n",
       " 'V70_na',\n",
       " 'V71_na',\n",
       " 'V72_na',\n",
       " 'V73_na',\n",
       " 'V74_na',\n",
       " 'V75_na',\n",
       " 'V76_na',\n",
       " 'V77_na',\n",
       " 'V78_na',\n",
       " 'V79_na',\n",
       " 'V8_na',\n",
       " 'V80_na',\n",
       " 'V81_na',\n",
       " 'V82_na',\n",
       " 'V83_na',\n",
       " 'V84_na',\n",
       " 'V85_na',\n",
       " 'V86_na',\n",
       " 'V87_na',\n",
       " 'V88_na',\n",
       " 'V89_na',\n",
       " 'V9_na',\n",
       " 'V90_na',\n",
       " 'V91_na',\n",
       " 'V92_na',\n",
       " 'V93_na',\n",
       " 'V94_na',\n",
       " 'V95_na',\n",
       " 'V96_na',\n",
       " 'V97_na',\n",
       " 'V98_na',\n",
       " 'V99_na',\n",
       " 'addr1_na',\n",
       " 'addr2_na',\n",
       " 'card2_na',\n",
       " 'card3_na',\n",
       " 'card5_na',\n",
       " 'dist1_na',\n",
       " 'dist2_na',\n",
       " 'id_01_na',\n",
       " 'id_02_na',\n",
       " 'id_03_na',\n",
       " 'id_04_na',\n",
       " 'id_05_na',\n",
       " 'id_06_na',\n",
       " 'id_07_na',\n",
       " 'id_08_na',\n",
       " 'id_09_na',\n",
       " 'id_10_na',\n",
       " 'id_11_na',\n",
       " 'id_13_na',\n",
       " 'id_14_na',\n",
       " 'id_17_na',\n",
       " 'id_18_na',\n",
       " 'id_19_na',\n",
       " 'id_20_na',\n",
       " 'id_21_na',\n",
       " 'id_22_na',\n",
       " 'id_24_na',\n",
       " 'id_25_na',\n",
       " 'id_26_na',\n",
       " 'id_32_na',\n",
       " 'C1_na',\n",
       " 'C10_na',\n",
       " 'C11_na',\n",
       " 'C12_na',\n",
       " 'C13_na',\n",
       " 'C14_na',\n",
       " 'C2_na',\n",
       " 'C3_na',\n",
       " 'C4_na',\n",
       " 'C5_na',\n",
       " 'C6_na',\n",
       " 'C7_na',\n",
       " 'C8_na',\n",
       " 'C9_na',\n",
       " 'D1_na',\n",
       " 'D10_na',\n",
       " 'D11_na',\n",
       " 'D12_na',\n",
       " 'D13_na',\n",
       " 'D14_na',\n",
       " 'D15_na',\n",
       " 'D2_na',\n",
       " 'D3_na',\n",
       " 'D4_na',\n",
       " 'D5_na',\n",
       " 'D6_na',\n",
       " 'D7_na',\n",
       " 'D8_na',\n",
       " 'D9_na',\n",
       " 'V1_na',\n",
       " 'V10_na',\n",
       " 'V100_na',\n",
       " 'V101_na',\n",
       " 'V102_na',\n",
       " 'V103_na',\n",
       " 'V104_na',\n",
       " 'V105_na',\n",
       " 'V106_na',\n",
       " 'V107_na',\n",
       " 'V108_na',\n",
       " 'V109_na',\n",
       " 'V11_na',\n",
       " 'V110_na',\n",
       " 'V111_na',\n",
       " 'V112_na',\n",
       " 'V113_na',\n",
       " 'V114_na',\n",
       " 'V115_na',\n",
       " 'V116_na',\n",
       " 'V117_na',\n",
       " 'V118_na',\n",
       " 'V119_na',\n",
       " 'V12_na',\n",
       " 'V120_na',\n",
       " 'V121_na',\n",
       " 'V122_na',\n",
       " 'V123_na',\n",
       " 'V124_na',\n",
       " 'V125_na',\n",
       " 'V126_na',\n",
       " 'V127_na',\n",
       " 'V128_na',\n",
       " 'V129_na',\n",
       " 'V13_na',\n",
       " 'V130_na',\n",
       " 'V131_na',\n",
       " 'V132_na',\n",
       " 'V133_na',\n",
       " 'V134_na',\n",
       " 'V135_na',\n",
       " 'V136_na',\n",
       " 'V137_na',\n",
       " 'V138_na',\n",
       " 'V139_na',\n",
       " 'V14_na',\n",
       " 'V140_na',\n",
       " 'V141_na',\n",
       " 'V142_na',\n",
       " 'V143_na',\n",
       " 'V144_na',\n",
       " 'V145_na',\n",
       " 'V146_na',\n",
       " 'V147_na',\n",
       " 'V148_na',\n",
       " 'V149_na',\n",
       " 'V15_na',\n",
       " 'V150_na',\n",
       " 'V151_na',\n",
       " 'V152_na',\n",
       " 'V153_na',\n",
       " 'V154_na',\n",
       " 'V155_na',\n",
       " 'V156_na',\n",
       " 'V157_na',\n",
       " 'V158_na',\n",
       " 'V159_na',\n",
       " 'V16_na',\n",
       " 'V160_na',\n",
       " 'V161_na',\n",
       " 'V162_na',\n",
       " 'V163_na',\n",
       " 'V164_na',\n",
       " 'V165_na',\n",
       " 'V166_na',\n",
       " 'V167_na',\n",
       " 'V168_na',\n",
       " 'V169_na',\n",
       " 'V17_na',\n",
       " 'V170_na',\n",
       " 'V171_na',\n",
       " 'V172_na',\n",
       " 'V173_na',\n",
       " 'V174_na',\n",
       " 'V175_na',\n",
       " 'V176_na',\n",
       " 'V177_na',\n",
       " 'V178_na',\n",
       " 'V179_na',\n",
       " 'V18_na',\n",
       " 'V180_na',\n",
       " 'V181_na',\n",
       " 'V182_na',\n",
       " 'V183_na',\n",
       " 'V184_na',\n",
       " 'V185_na',\n",
       " 'V186_na',\n",
       " 'V187_na',\n",
       " 'V188_na',\n",
       " 'V189_na',\n",
       " 'V19_na',\n",
       " 'V190_na',\n",
       " 'V191_na',\n",
       " 'V192_na',\n",
       " 'V193_na',\n",
       " 'V194_na',\n",
       " 'V195_na',\n",
       " 'V196_na',\n",
       " 'V197_na',\n",
       " 'V198_na',\n",
       " 'V199_na',\n",
       " 'V2_na',\n",
       " 'V20_na',\n",
       " 'V200_na',\n",
       " 'V201_na',\n",
       " 'V202_na',\n",
       " 'V203_na',\n",
       " 'V204_na',\n",
       " 'V205_na',\n",
       " 'V206_na',\n",
       " 'V207_na',\n",
       " 'V208_na',\n",
       " 'V209_na',\n",
       " 'V21_na',\n",
       " 'V210_na',\n",
       " 'V211_na',\n",
       " 'V212_na',\n",
       " 'V213_na',\n",
       " 'V214_na',\n",
       " 'V215_na',\n",
       " 'V216_na',\n",
       " 'V217_na',\n",
       " 'V218_na',\n",
       " 'V219_na',\n",
       " 'V22_na',\n",
       " 'V220_na',\n",
       " 'V221_na',\n",
       " ...]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          4\n",
       "2          7\n",
       "3          8\n",
       "4          4\n",
       "          ..\n",
       "1097226    4\n",
       "1097227    5\n",
       "1097228    5\n",
       "1097229    5\n",
       "1097230    5\n",
       "Name: P_emaildomain, Length: 1097231, dtype: category\n",
       "Categories (9, int64): [0 < 1 < 2 < 3 ... 5 < 6 < 7 < 8]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['P_emaildomain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's auc: 0.907658\tvalid_1's auc: 0.828865\n",
      "[200]\ttraining's auc: 0.933838\tvalid_1's auc: 0.838053\n",
      "[300]\ttraining's auc: 0.954901\tvalid_1's auc: 0.842877\n",
      "[400]\ttraining's auc: 0.969734\tvalid_1's auc: 0.843728\n",
      "[500]\ttraining's auc: 0.978241\tvalid_1's auc: 0.842807\n",
      "[600]\ttraining's auc: 0.98403\tvalid_1's auc: 0.843003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-3c0bf1620648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], \n\u001b[0;32m---> 30\u001b[0;31m                 verbose_eval=100, early_stopping_rounds=1000)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \"\"\"\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   2302\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_out_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m                 result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m   2305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_out_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_inner_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length of eval results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 500,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47\n",
    "         }\n",
    "\n",
    "params = {'num_leaves': 500,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "         }\n",
    "\n",
    "clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], \n",
    "                verbose_eval=100, early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_latent_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.data.add_test(TabularList.from_df(df_latent_test, cat_names=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({'TransactionID' : test_ids, 'isFraud' : preds[0][:,1]})\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'cv{cv}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'ieee-fraud-detection'\n",
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m '{cv}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
